<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- jQuery and face-api -->
  <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.4.2/jquery.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

  <!-- Mediapipe (Camera, FaceMesh, Pose) -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils_3d/control_utils_3d.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/pose/pose.js" crossorigin="anonymous"></script>

  <!-- TensorFlow.js -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.1"></script>
  <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>

  <style>
    .container {
      display: flex;
      align-items: center;
      justify-content: space-evenly;
    }
    .output_canvas, .output_canvas2 {
      border: 10px solid #7397da;
      border-radius: 10px;
      width: 40%;
    }
    .pred {
      display: flex;
      flex-direction: row;
      align-items: center;
      justify-content: center;
    }
    table {
      width: 90%;
      margin: 20px auto;
      border-collapse: collapse;
    }
    table, th, td {
      border: 1px solid #333;
    }
    th, td {
      padding: 8px;
      text-align: center;
    }
  </style>

  <title>Face & Pose with Table</title>
</head>

<body>
  <h1>Face Recognition Page</h1>
  <div id="status">Status here...</div>

  <!-- Shared video + two canvases -->
  <div class="container">
    <video class="input_video" id="videoElement" style="display:none;" autoplay muted playsinline></video>
    <canvas class="output_canvas" id="faceCanvas" width="640" height="480"></canvas>
    <canvas class="output_canvas2" id="poseCanvas" width="640" height="480"></canvas>
  </div>

  <!-- Face + Pose labels -->
  <div class="pred">
    <h1 id="labelOnVideo" style="margin-top:10px;">Face</h1>
    <h1 id="prediction" style="margin-top:10px;">Pose</h1>
  </div>

  <!-- Attendance Table -->
  <table id="attendanceTable">
    <thead>
      <tr>
        <th>Name</th>
        <th>Checkin Time</th>
        <th>Checkout Time</th>
        <th>Phone Duration (sec)</th>
        <th>Looking Away Count</th>
        <th>Working Duration (sec)</th>
      </tr>
    </thead>
    <tbody></tbody>
  </table>

  <!-- Data + Helper Code -->
  <script>
    let currentFaceName = null;
    let attendanceRecords = {};

    function formatTimestamp(ts) {
      return new Date(ts).toLocaleString();
    }

    function updateAttendanceTable() {
      const tableBody = document.querySelector("#attendanceTable tbody");
      tableBody.innerHTML = "";
      for (const key in attendanceRecords) {
        const rec = attendanceRecords[key];
        const row = document.createElement("tr");
        row.innerHTML = `
          <td>${rec.name}</td>
          <td>${formatTimestamp(rec.checkin)}</td>
          <td>${formatTimestamp(rec.checkout)}</td>
          <td>${rec.phoneDuration.toFixed(1)}</td>
          <td>${rec.lookingAwayCount}</td>
          <td>${rec.workingDuration.toFixed(1)}</td>
        `;
        tableBody.appendChild(row);
      }
    }
  </script>

  <!-- FACE RECOGNITION & FACE MESH -->
  <script>
    let faceMatcher = null; 
    let videoStream = null;
    const videoEl = document.getElementById('videoElement');
    const faceCanvas = document.getElementById('faceCanvas');
    const faceCtx = faceCanvas.getContext('2d');
    const labelOnVideoEl = document.getElementById('labelOnVideo');
    const statusEl = document.getElementById('status');

    // Load face-api models on DOM load
    document.addEventListener('DOMContentLoaded', async () => {
      try {
        statusEl.textContent = 'Loading face-api models...';
        await faceapi.nets.tinyFaceDetector.loadFromUri('./models');
        await faceapi.nets.faceLandmark68Net.loadFromUri('./models');
        await faceapi.nets.faceRecognitionNet.loadFromUri('./models');
        statusEl.textContent = 'Face-API models loaded. Starting webcam...';

        // Get webcam
        videoStream = await navigator.mediaDevices.getUserMedia({ video: true });
        videoEl.srcObject = videoStream;

        // When webcam is ready, start face recognition
        videoEl.onloadedmetadata = () => {
          videoEl.play();
          statusEl.textContent = 'Webcam ready. Building face matcher...';
          buildFaceMatcher();
          recognitionLoop();
        };
      } catch (err) {
        console.error(err);
        statusEl.textContent = 'Error: ' + err;
      }
    });

    function buildFaceMatcher() {
      const stored = localStorage.getItem('faceDescriptors');
      if (!stored) {
        statusEl.textContent = 'No stored faces found. Please train first.';
        return;
      }
      const parsed = JSON.parse(stored);
      const labeledDescriptors = [];
      for (const label in parsed) {
        const descriptorArray = parsed[label];
        const float32s = descriptorArray.map(arr => new Float32Array(arr));
        labeledDescriptors.push(new faceapi.LabeledFaceDescriptors(label, float32s));
      }
      faceMatcher = new faceapi.FaceMatcher(labeledDescriptors, 0.6);
      statusEl.textContent = 'Face matcher ready.';
    }

    // Face recognition loop
    async function recognitionLoop() {
      requestAnimationFrame(recognitionLoop);
      if (!faceMatcher) return;
      try {
        const detections = await faceapi
          .detectAllFaces(videoEl, new faceapi.TinyFaceDetectorOptions())
          .withFaceLandmarks()
          .withFaceDescriptors();

        if (!detections.length) {
          labelOnVideoEl.textContent = 'No face';
          currentFaceName = null;
        } else {
          const bestMatch = faceMatcher.findBestMatch(detections[0].descriptor);
          labelOnVideoEl.textContent = bestMatch.toString(); 
          currentFaceName = bestMatch.label;
        }
      } catch (err) {
        console.error('Face recognition error:', err);
      }
    }

    // FaceMesh overlay
    function onFaceMeshResults(results) {
      faceCtx.save();
      faceCtx.clearRect(0, 0, faceCanvas.width, faceCanvas.height);
      faceCtx.drawImage(results.image, 0, 0, faceCanvas.width, faceCanvas.height);
      if (results.multiFaceLandmarks) {
        for (const lm of results.multiFaceLandmarks) {
          drawConnectors(faceCtx, lm, FACEMESH_TESSELATION, {color: '#C0C0C070', lineWidth: 1});
          drawConnectors(faceCtx, lm, FACEMESH_RIGHT_EYE, {color: '#FF3030'});
          drawConnectors(faceCtx, lm, FACEMESH_RIGHT_EYEBROW, {color: '#FF3030'});
          drawConnectors(faceCtx, lm, FACEMESH_RIGHT_IRIS, {color: '#FF3030'});
          drawConnectors(faceCtx, lm, FACEMESH_LEFT_EYE, {color: '#30FF30'});
          drawConnectors(faceCtx, lm, FACEMESH_LEFT_EYEBROW, {color: '#30FF30'});
          drawConnectors(faceCtx, lm, FACEMESH_LEFT_IRIS, {color: '#30FF30'});
          drawConnectors(faceCtx, lm, FACEMESH_FACE_OVAL, {color: '#E0E0E0'});
          drawConnectors(faceCtx, lm, FACEMESH_LIPS, {color: '#E0E0E0'});
        }
      }
      faceCtx.restore();
    }

    const faceMesh = new FaceMesh({
      locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
    });
    faceMesh.setOptions({
      maxNumFaces: 1,
      refineLandmarks: true,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5
    });
    faceMesh.onResults(onFaceMeshResults);
  </script>

  <!-- POSE & ATTENDANCE -->
  <script>
    let poseModel = null;
    const poseCanvas = document.getElementById('poseCanvas');
    const poseCtx = poseCanvas.getContext('2d');
    const predictionDiv = document.getElementById('prediction');

    // Load pose TFJS model
    async function loadPoseModel() {
      const modelURL = 'tfjs_model/model.json';
      poseModel = await tf.loadGraphModel(modelURL);
      console.log("Pose model loaded");
    }

    // Pose
    const pose = new Pose({
      locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/pose/${file}`
    });
    pose.setOptions({
      modelComplexity: 1,
      smoothLandmarks: true,
      enableSegmentation: false,
      smoothSegmentation: false,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5
    });
    pose.onResults(onPoseResults);

    async function onPoseResults(results) {
      poseCtx.save();
      poseCtx.clearRect(0, 0, poseCanvas.width, poseCanvas.height);
      poseCtx.drawImage(results.image, 0, 0, poseCanvas.width, poseCanvas.height);

      if (!results.poseLandmarks) {
        predictionDiv.textContent = "No Pose";
        poseCtx.restore();
        return;
      }
      drawConnectors(poseCtx, results.poseLandmarks, POSE_CONNECTIONS, { color: '#00FF00', lineWidth: 4 });
      drawLandmarks(poseCtx, results.poseLandmarks, { color: '#FF0000', lineWidth: 2 });
      poseCtx.restore();

      if (!poseModel) return;
      const arr = [];
      results.poseLandmarks.forEach(lm => {
        arr.push(lm.x, lm.y, lm.z, lm.visibility);
      });
      const inputTensor = tf.tensor(arr, [1, arr.length]);

      try {
        const predTensor = await poseModel.predict(inputTensor);
        const predData = await predTensor.data();
        const maxIndex = predData.indexOf(Math.max(...predData));
        const classLabels = ['looking_away', 'phone', 'working'];
        const predictedClass = classLabels[maxIndex] || "Unknown";
        predictionDiv.textContent = predictedClass;

        // Update attendance
        if (currentFaceName && currentFaceName.toLowerCase() !== "unknown") {
          let now = Date.now();
          let dateKey = new Date().toISOString().split('T')[0];
          let recordKey = currentFaceName + "_" + dateKey;

          if (!attendanceRecords[recordKey]) {
            attendanceRecords[recordKey] = {
              name: currentFaceName,
              checkin: now,
              checkout: now,
              phoneDuration: 0,
              workingDuration: 0,
              lookingAwayCount: 0,
              lastPhoneUpdate: now,
              lastWorkingUpdate: now,
              lastPose: null
            };
          } else {
            attendanceRecords[recordKey].checkout = now;
          }
          let rec = attendanceRecords[recordKey];
          if (predictedClass === "phone") {
            if (rec.lastPose !== "phone") {
              rec.lastPhoneUpdate = now;
            } else {
              let delta = (now - rec.lastPhoneUpdate) / 1000;
              rec.phoneDuration += delta;
              rec.lastPhoneUpdate = now;
            }
          }
          if (predictedClass === "working") {
            if (rec.lastPose !== "working") {
              rec.lastWorkingUpdate = now;
            } else {
              let delta = (now - rec.lastWorkingUpdate) / 1000;
              rec.workingDuration += delta;
              rec.lastWorkingUpdate = now;
            }
          }
          if (predictedClass === "looking_away") {
            if (rec.lastPose !== "looking_away") {
              rec.lookingAwayCount += 1;
            }
          }
          rec.lastPose = predictedClass;
          updateAttendanceTable();
        }
        inputTensor.dispose();
        predTensor.dispose();
      } catch (err) {
        console.error("Pose prediction error:", err);
      }
    }

    // One camera for both faceMesh and Pose
    const mpCamera = new Camera(videoEl, {
      onFrame: async () => {
        await faceMesh.send({ image: videoEl });
        await pose.send({ image: videoEl });
      },
      width: 640,
      height: 480
    });

    // Load the pose model and start
    window.addEventListener('load', async () => {
      await loadPoseModel();
      mpCamera.start();
      console.log("Mediapipe camera started");
    });
  </script>
</body>
</html>
