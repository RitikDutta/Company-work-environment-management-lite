<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- jQuery and face-api -->
    <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.4.2/jquery.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

    <!-- Mediapipe: Camera, FaceMesh, Pose, etc. -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils_3d/control_utils_3d.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/pose/pose.js" crossorigin="anonymous"></script>

    <!-- TensorFlow.js -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.1"></script>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>

    <style>
      .container {
        display: flex;
        align-items: center;
        justify-content: space-evenly;
      }
      .output_canvas, .output_canvas2 {
        border: 10px solid #7397da;
        border-radius: 10px;
        width: 40%;
      }
      .text3 {
        text-align: center;
      }
      table {
        width: 90%;
        margin: 20px auto;
        border-collapse: collapse;
      }
      table, th, td {
        border: 1px solid #333;
      }
      th, td {
        padding: 8px;
        text-align: center;
      }
      .pred {
        display: flex;
        flex-direction: row;
        align-items: center;
        justify-content: center;
      }
    </style>
    <title>Face Recognition, Pose & Attendance</title>
  </head>
  <body>
    <h1>Face Recognition Page</h1>
    <div id="status">Status messages will appear here...</div>

    <!-- Container: shared video and two canvases -->
    <div class="container">
      <!-- Hidden video shared by both systems -->
      <video class="input_video" id="videoElement" style="display: none;" autoplay muted playsinline></video>
      <!-- Left: Face recognition & FaceMesh -->
      <canvas class="output_canvas" id="faceCanvas" width="640" height="480"></canvas>
      <!-- Right: Pose prediction -->
      <canvas class="output_canvas2" id="poseCanvas" width="640" height="480"></canvas>
    </div>
    <div class="pred">
      <h1 id="labelOnVideo" style="text-align:center; margin-top:10px;" class="text3">Face-</h1>
      <h1 id="prediction" style="text-align:center; margin-top:10px;">Pose</h1>
    </div>

    <!-- Attendance Table -->
    <table id="attendanceTable">
      <thead>
        <tr>
          <th>Name</th>
          <th>Checkin Time</th>
          <th>Checkout Time</th>
          <th>Phone Duration (sec)</th>
          <th>Looking Away Count</th>
          <th>Working Duration (sec)</th>
        </tr>
      </thead>
      <tbody>
        <!-- Rows added dynamically -->
      </tbody>
    </table>

    <!-- GLOBAL VARIABLES & HELPER FUNCTIONS -->
    <script>
      // Current recognized face
      var currentFaceName = null;
      // Attendance records: key = "Name_Date"
      var attendanceRecords = {};

      // Format a timestamp nicely.
      function formatTimestamp(ts) {
        return new Date(ts).toLocaleString();
      }

      // Refresh the attendance table.
      function updateAttendanceTable() {
        const tableBody = document.querySelector("#attendanceTable tbody");
        tableBody.innerHTML = ""; // Clear rows
        for (const key in attendanceRecords) {
          const rec = attendanceRecords[key];
          const row = document.createElement("tr");
          row.innerHTML = `
            <td>${rec.name}</td>
            <td>${formatTimestamp(rec.checkin)}</td>
            <td>${formatTimestamp(rec.checkout)}</td>
            <td>${rec.phoneDuration.toFixed(1)}</td>
            <td>${rec.lookingAwayCount}</td>
            <td>${rec.workingDuration.toFixed(1)}</td>
          `;
          tableBody.appendChild(row);
        }
      }
    </script>

    <!-- FACE RECOGNITION & FACE MESH CODE -->
    <script>
      let modelsLoaded = false;
      let faceMatcher = null;
      let videoStream = null;

      const statusEl = document.getElementById("status");
      const videoEl = document.getElementById("videoElement");
      const labelOnVideoEl = document.getElementById("labelOnVideo");

      // Load face-api models and get video stream.
      async function initFaceApi() {
        statusEl.textContent = "Loading face models...";
        try {
          await faceapi.nets.tinyFaceDetector.loadFromUri("./models");
          await faceapi.nets.faceLandmark68Net.loadFromUri("./models");
          await faceapi.nets.faceRecognitionNet.loadFromUri("./models");
          modelsLoaded = true;
          statusEl.textContent = "Face models loaded.";
        } catch (err) {
          statusEl.textContent = "Error loading face models: " + err;
          console.error(err);
          return;
        }

        try {
          videoStream = await navigator.mediaDevices.getUserMedia({ video: true });
          videoEl.srcObject = videoStream;
        } catch (err) {
          console.error("Webcam error: ", err);
          statusEl.textContent = "Error accessing webcam: " + err;
          return;
        }

        // Wait for video to be ready.
        await new Promise((resolve) => {
          videoEl.onloadedmetadata = () => {
            videoEl.play();
            resolve();
          };
        });
        statusEl.textContent = "Video started.";
      }

      // Create face matcher from stored descriptors.
      async function createMatcherAndStartRecognition() {
        const stored = localStorage.getItem("faceDescriptors");
        if (!stored) {
          statusEl.textContent = "No face descriptors found. Please train first.";
          console.log("No descriptors in localStorage");
          return;
        }
        const parsed = JSON.parse(stored);
        const labeledDescriptors = [];
        for (const label in parsed) {
          const descriptorArray = parsed[label];
          const float32s = descriptorArray.map((arr) => new Float32Array(arr));
          labeledDescriptors.push(new faceapi.LabeledFaceDescriptors(label, float32s));
        }
        faceMatcher = new faceapi.FaceMatcher(labeledDescriptors, 0.6);
        statusEl.textContent = "Matcher created. Starting face recognition...";
        recognitionLoop();
      }

      // Loop for face recognition.
      async function recognitionLoop() {
        requestAnimationFrame(recognitionLoop);
        if (!faceMatcher) return;
        try {
          const detections = await faceapi
            .detectAllFaces(videoEl, new faceapi.TinyFaceDetectorOptions())
            .withFaceLandmarks()
            .withFaceDescriptors();
          if (!detections.length) {
            labelOnVideoEl.textContent = "No face detected";
            currentFaceName = null;
          } else {
            const bestMatch = faceMatcher.findBestMatch(detections[0].descriptor);
            labelOnVideoEl.textContent = bestMatch.toString();
            currentFaceName = bestMatch.label;
          }
        } catch (err) {
          console.error("Face recognition error:", err);
        }
      }

      // FaceMesh to overlay face landmarks.
      const faceCanvas = document.getElementById("faceCanvas");
      const faceCtx = faceCanvas.getContext("2d");
      function onFaceMeshResults(results) {
        faceCtx.save();
        faceCtx.clearRect(0, 0, faceCanvas.width, faceCanvas.height);
        faceCtx.drawImage(results.image, 0, 0, faceCanvas.width, faceCanvas.height);
        if (results.multiFaceLandmarks) {
          for (const landmarks of results.multiFaceLandmarks) {
            drawConnectors(faceCtx, landmarks, FACEMESH_TESSELATION, { color: "#C0C0C070", lineWidth: 1 });
            drawConnectors(faceCtx, landmarks, FACEMESH_RIGHT_EYE, { color: "#FF3030" });
            drawConnectors(faceCtx, landmarks, FACEMESH_RIGHT_EYEBROW, { color: "#FF3030" });
            drawConnectors(faceCtx, landmarks, FACEMESH_RIGHT_IRIS, { color: "#FF3030" });
            drawConnectors(faceCtx, landmarks, FACEMESH_LEFT_EYE, { color: "#30FF30" });
            drawConnectors(faceCtx, landmarks, FACEMESH_LEFT_EYEBROW, { color: "#30FF30" });
            drawConnectors(faceCtx, landmarks, FACEMESH_LEFT_IRIS, { color: "#30FF30" });
            drawConnectors(faceCtx, landmarks, FACEMESH_FACE_OVAL, { color: "#E0E0E0" });
            drawConnectors(faceCtx, landmarks, FACEMESH_LIPS, { color: "#E0E0E0" });
          }
        }
        faceCtx.restore();
      }
      const faceMesh = new FaceMesh({
        locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`,
      });
      faceMesh.setOptions({
        maxNumFaces: 1,
        refineLandmarks: true,
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5,
      });
      faceMesh.onResults(onFaceMeshResults);
    </script>

    <!-- POSE PREDICTION & ATTENDANCE TRACKING -->
    <script>
      let poseModel; // TF.js pose model
      const poseCanvas = document.getElementById("poseCanvas");
      const poseCtx = poseCanvas.getContext("2d");
      const predictionDiv = document.getElementById("prediction");

      // Load the pose model.
      async function loadPoseModel() {
        const modelURL = "tfjs_model/model.json";
        console.log("Loading pose model from", modelURL);
        try {
          poseModel = await tf.loadGraphModel(modelURL);
          console.log("Pose model loaded.");
        } catch (error) {
          console.error("Error loading pose model:", error);
        }
      }

      // Mediapipe Pose setup.
      const pose = new Pose({
        locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/pose/${file}`,
      });
      pose.setOptions({
        modelComplexity: 1,
        smoothLandmarks: true,
        enableSegmentation: false,
        smoothSegmentation: false,
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5,
      });
      pose.onResults(onPoseResults);

      // Process pose results and update attendance.
      async function onPoseResults(results) {
        poseCtx.save();
        poseCtx.clearRect(0, 0, poseCanvas.width, poseCanvas.height);
        poseCtx.drawImage(results.image, 0, 0, poseCanvas.width, poseCanvas.height);

        if (!results.poseLandmarks) {
          predictionDiv.innerText = "Pose: No pose detected";
          poseCtx.restore();
          return;
        }
        // Draw pose landmarks.
        window.drawConnectors(poseCtx, results.poseLandmarks, window.POSE_CONNECTIONS, { color: "#00FF00", lineWidth: 4 });
        window.drawLandmarks(poseCtx, results.poseLandmarks, { color: "#FF0000", lineWidth: 2 });
        poseCtx.restore();

        // Create input array for the model.
        let inputArray = [];
        results.poseLandmarks.forEach((lm) => {
          inputArray.push(lm.x, lm.y, lm.z, lm.visibility);
        });
        console.log("Pose input length:", inputArray.length);
        if (!poseModel) return;
        const inputTensor = tf.tensor(inputArray, [1, inputArray.length]);

        try {
          const predTensor = await poseModel.predict(inputTensor);
          const predData = await predTensor.data();
          const maxIndex = predData.indexOf(Math.max(...predData));
          const classLabels = ["looking_away", "phone", "working"];
          const predictedClass = classLabels[maxIndex] || "Unknown";
          predictionDiv.innerText = "Pose: " + predictedClass;

          // Update attendance only if a known face is detected.
          if (currentFaceName && currentFaceName.toLowerCase() !== "unknown") {
            let now = Date.now();
            // Use date (YYYY-MM-DD) as key.
            let dateKey = new Date().toISOString().split("T")[0];
            let recordKey = currentFaceName + "_" + dateKey;
            if (!attendanceRecords[recordKey]) {
              // Create new record.
              attendanceRecords[recordKey] = {
                name: currentFaceName,
                date: dateKey,
                checkin: now,
                checkout: now,
                phoneDuration: 0,
                workingDuration: 0,
                lookingAwayCount: 0,
                lastPhoneUpdate: now,
                lastWorkingUpdate: now,
                lastPose: null,
              };
            } else {
              // Update checkout time.
              attendanceRecords[recordKey].checkout = now;
            }
            let rec = attendanceRecords[recordKey];
            // Update phone duration.
            if (predictedClass === "phone") {
              if (rec.lastPose !== "phone") {
                rec.lastPhoneUpdate = now;
              } else {
                let delta = (now - rec.lastPhoneUpdate) / 1000;
                rec.phoneDuration += delta;
                rec.lastPhoneUpdate = now;
              }
            }
            // Update working duration.
            if (predictedClass === "working") {
              if (rec.lastPose !== "working") {
                rec.lastWorkingUpdate = now;
              } else {
                let delta = (now - rec.lastWorkingUpdate) / 1000;
                rec.workingDuration += delta;
                rec.lastWorkingUpdate = now;
              }
            }
            // Count looking away events.
            if (predictedClass === "looking_away") {
              if (rec.lastPose !== "looking_away") {
                rec.lookingAwayCount += 1;
              }
            }
            rec.lastPose = predictedClass;
            updateAttendanceTable();
          }

          inputTensor.dispose();
          predTensor.dispose();
        } catch (err) {
          console.error("Pose prediction error:", err);
        }
      }
    </script>

    <!-- INITIALIZATION: Wait for video, load models, then start camera -->
    <script>
      async function init() {
        // 1. Load face-api models & get video stream.
        await initFaceApi();
        // 2. Start face recognition.
        await createMatcherAndStartRecognition();
        // 3. Load the pose model.
        await loadPoseModel();
        // 4. Create and start the camera.
        const camera = new Camera(videoEl, {
          onFrame: async () => {
            // Feed one camera to both FaceMesh and Pose.
            await faceMesh.send({ image: videoEl });
            await pose.send({ image: videoEl });
          },
          width: 640,
          height: 480,
        });
        statusEl.textContent = "All systems ready. Camera started.";
        camera.start();
      }

      // Start everything once the window loads.
      window.addEventListener("load", init);
    </script>
  </body>
</html>
