<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Face Recognition Page</title>
  <!-- face-api.js from CDN -->
  <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
  <style>
    body {
      font-family: Arial, sans-serif;
      text-align: center;
      margin: 20px;
    }
    #status {
      margin-top: 10px;
      font-weight: bold;
      color: green;
    }
    .container {
      position: relative;
      display: inline-block;
      width: 640px;
      height: 480px;
      border: 1px solid #ccc;
    }
    #videoElement {
      position: absolute;
      top: 0;
      left: 0;
      width: 640px;
      height: 480px;
      background: #000;
      z-index: 0;
    }
    #labelOnVideo {
      position: absolute;
      top: 0; 
      left: 0;
      width: 640px;
      height: auto;
      color: yellow;
      font-weight: bold;
      text-align: center;
      background: rgba(0,0,0,0.3);
      z-index: 10;
      pointer-events: none; /* clicks pass through */
    }
    #controls {
      margin-bottom: 1rem;
    }
  </style>
</head>
<body>
  <h1>Face Recognition Page</h1>

  <div id="controls">
    <button id="loadModelsBtn">Load Models</button>
    <button id="startVideoBtn">Start Video</button>
    <button id="createMatcherBtn">Create Matcher</button>
    <button id="startRecBtn">Start Recognition</button>
  </div>

  <div id="status">Status messages will appear here...</div>

  <div class="container">
    <video id="videoElement" autoplay muted playsinline></video>
    <div id="labelOnVideo"></div>
  </div>

  <script>
    // ------------------------------------------------------------------
    // GLOBAL STATE
    // ------------------------------------------------------------------
    let modelsLoaded = false;
    let videoStream = null;
    let faceMatcher = null; // We'll build from localStorage data

    // ------------------------------------------------------------------
    // DOM ELEMENTS
    // ------------------------------------------------------------------
    const loadModelsBtn    = document.getElementById('loadModelsBtn');
    const startVideoBtn    = document.getElementById('startVideoBtn');
    const createMatcherBtn = document.getElementById('createMatcherBtn');
    const startRecBtn      = document.getElementById('startRecBtn');
    const statusEl         = document.getElementById('status');
    const videoEl          = document.getElementById('videoElement');
    const labelOnVideoEl   = document.getElementById('labelOnVideo');

    // ------------------------------------------------------------------
    // 1. LOAD MODELS
    // ------------------------------------------------------------------
    loadModelsBtn.addEventListener('click', async () => {
      statusEl.textContent = 'Loading models...';
      try {
        await faceapi.nets.tinyFaceDetector.loadFromUri('./models');
        await faceapi.nets.faceLandmark68Net.loadFromUri('./models');
        await faceapi.nets.faceRecognitionNet.loadFromUri('./models');
        modelsLoaded = true;
        statusEl.textContent = 'Models loaded successfully!';
        console.log('[DEBUG] Models loaded');
      } catch (err) {
        statusEl.textContent = 'Error loading models: ' + err;
        console.error(err);
      }
    });

    // ------------------------------------------------------------------
    // 2. START VIDEO
    // ------------------------------------------------------------------
    startVideoBtn.addEventListener('click', async () => {
      if (!modelsLoaded) {
        statusEl.textContent = 'Please load models first!';
        return;
      }
      if (videoStream) {
        statusEl.textContent = 'Video is already running.';
        return;
      }
      try {
        videoStream = await navigator.mediaDevices.getUserMedia({ video: true });
        videoEl.srcObject = videoStream;
        videoEl.onloadedmetadata = () => {
          videoEl.play();
          statusEl.textContent = 'Video stream started.';
          console.log('[DEBUG] Video started');
        };
      } catch (err) {
        console.error(err);
        statusEl.textContent = 'Error accessing webcam: ' + err;
      }
    });

    // ------------------------------------------------------------------
    // 3. CREATE MATCHER FROM LOCALSTORAGE
    // ------------------------------------------------------------------
    createMatcherBtn.addEventListener('click', () => {
      // Retrieve stored descriptors from localStorage
      const stored = localStorage.getItem('faceDescriptors');
      if (!stored) {
        statusEl.textContent = 'No descriptors found in localStorage! Train first.';
        console.log('[DEBUG] No descriptors in localStorage');
        return;
      }

      // Parse the JSON
      const parsed = JSON.parse(stored);
      // Example structure: { "Alice": [ [0.1, 0.2, ...], ... ], "Bob": [...], ... }

      // Rebuild them into LabeledFaceDescriptors
      const labeledDescriptors = [];
      for (const label in parsed) {
        const descriptorArray = parsed[label]; // e.g. [ [0.1, 0.2, ...], [ ... ] ]
        const float32Arrays = descriptorArray.map(arr => new Float32Array(arr));
        labeledDescriptors.push(
          new faceapi.LabeledFaceDescriptors(label, float32Arrays)
        );
      }

      // Create FaceMatcher
      faceMatcher = new faceapi.FaceMatcher(labeledDescriptors, 0.6);
      statusEl.textContent = 'FaceMatcher created (threshold=0.6) from localStorage.';
      console.log('[DEBUG] FaceMatcher created from localStorage');
    });

    // ------------------------------------------------------------------
    // 4. START RECOGNITION LOOP
    // ------------------------------------------------------------------
    startRecBtn.addEventListener('click', () => {
      if (!faceMatcher) {
        statusEl.textContent = 'Please create matcher first!';
        return;
      }
      statusEl.textContent = 'Recognition running. Label displayed on top of the video.';
      console.log('[DEBUG] Recognition started');
      recognitionLoop();
    });

    async function recognitionLoop() {
      requestAnimationFrame(recognitionLoop);

      try {
        const detections = await faceapi.detectAllFaces(
          videoEl,
          new faceapi.TinyFaceDetectorOptions()
        ).withFaceLandmarks().withFaceDescriptors();

        if (!detections.length) {
          labelOnVideoEl.textContent = 'No face detected';
        } else {
          // For simplicity, match the first face
          const bestMatch = faceMatcher.findBestMatch(detections[0].descriptor);
          labelOnVideoEl.textContent = `Detected: ${bestMatch.toString()}`;
        }
      } catch (err) {
        console.error('Detection error:', err);
      }
    }
  </script>
</body>
</html>
