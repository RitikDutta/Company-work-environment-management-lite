<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>face-api.js Example (Inspired by the Blog Post)</title>
  <!-- Load face-api.js from a CDN -->
  <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 20px;
      text-align: center;
    }
    #inputImage, #overlay, #videoPreview {
      border: 1px solid #ccc;
      max-width: 640px;
      max-height: 480px;
      margin-top: 10px;
    }
    #overlay {
      position: absolute;
      top: 0;
      left: 0;
      pointer-events: none;
      z-index: 999;
    }
    .container {
      position: relative;
      display: inline-block;
    }
    #controls {
      margin-bottom: 1rem;
    }
    #status {
      margin-top: 0.5rem;
      color: #009900;
      font-weight: bold;
    }
  </style>
</head>
<body>
  <h1>Face Recognition in the Browser</h1>
  <p>Inspired by the face-api.js blog post</p>

  <div id="controls">
    <button id="loadModelsBtn">Load Models</button>
    <button id="detectImageBtn">Detect in Image</button>
    <button id="startVideoBtn">Start Video</button>
    <button id="registerFaceBtn">Register Face</button>
    <button id="createMatcherBtn">Create Matcher</button>
    <button id="recognizeBtn">Recognize (Video)</button>
  </div>

  <div>
    <input type="file" id="imageUpload" accept="image/*" />
    <input type="text" id="labelInput" placeholder="Enter label" />
  </div>

  <div id="status">Status messages go here...</div>

  <!-- Container for image + overlay -->
  <div class="container">
    <img id="inputImage" src="" alt="Selected Image" />
    <canvas id="overlay"></canvas>
  </div>

  <!-- Container for video preview -->
  <div class="container">
    <video id="videoPreview" autoplay muted playsinline width="640" height="480"></video>
    <canvas id="videoOverlay" width="640" height="480"></canvas>
  </div>

  <script>
    // -------------------------------------------------------
    // Global Variables
    // -------------------------------------------------------
    let faceMatcher = null;                  // Will hold FaceMatcher
    let labeledDescriptors = [];             // Array of face-api.LabeledFaceDescriptors
    let modelsLoaded = false;
    let videoStream = null;

    // We'll store all face descriptors here for any new label
    // { 'Alice': [descriptor1, descriptor2, ...], 'Bob': [...] }
    let descriptorsByLabel = {};

    // -------------------------------------------------------
    // DOM Elements
    // -------------------------------------------------------
    const statusEl       = document.getElementById('status');
    const loadModelsBtn  = document.getElementById('loadModelsBtn');
    const detectImageBtn = document.getElementById('detectImageBtn');
    const startVideoBtn  = document.getElementById('startVideoBtn');
    const registerFaceBtn= document.getElementById('registerFaceBtn');
    const createMatcherBtn = document.getElementById('createMatcherBtn');
    const recognizeBtn   = document.getElementById('recognizeBtn');

    const imageUploadEl  = document.getElementById('imageUpload');
    const inputImageEl   = document.getElementById('inputImage');
    const overlayEl      = document.getElementById('overlay');
    const videoPreviewEl = document.getElementById('videoPreview');
    const videoOverlayEl = document.getElementById('videoOverlay');
    const labelInputEl   = document.getElementById('labelInput');

    // -------------------------------------------------------
    // 1. Load Models
    // -------------------------------------------------------
    loadModelsBtn.addEventListener('click', async () => {
      statusEl.textContent = 'Loading models...';
      try {
        await faceapi.nets.tinyFaceDetector.loadFromUri('./models');
        await faceapi.nets.faceLandmark68Net.loadFromUri('./models');
        await faceapi.nets.faceRecognitionNet.loadFromUri('./models');
        // If you'd like to use SSD MobileNet instead of Tiny Face Detector:
        // await faceapi.nets.ssdMobilenetv1.loadFromUri('./models');

        modelsLoaded = true;
        statusEl.textContent = 'Models loaded successfully!';
      } catch (err) {
        statusEl.textContent = 'Error loading models: ' + err;
        console.error(err);
      }
    });

    // -------------------------------------------------------
    // 2. Detect in an Uploaded Image
    // -------------------------------------------------------
    imageUploadEl.addEventListener('change', () => {
      // Display selected image in <img> tag
      const file = imageUploadEl.files[0];
      if (!file) return;
      const reader = new FileReader();
      reader.onload = (e) => {
        inputImageEl.src = e.target.result;
      };
      reader.readAsDataURL(file);
    });

    detectImageBtn.addEventListener('click', async () => {
      if (!modelsLoaded) {
        statusEl.textContent = 'Please load models first!';
        return;
      }
      if (!inputImageEl.src) {
        alert('Please select an image first!');
        return;
      }

      // Wait for the image to load
      await inputImageEl.decode();
      statusEl.textContent = 'Detecting faces in image...';

      // Prepare overlay
      const canvas = overlayEl;
      const displaySize = { width: inputImageEl.width, height: inputImageEl.height };
      canvas.width = displaySize.width;
      canvas.height = displaySize.height;

      // Detect faces with descriptors
      const detections = await faceapi.detectAllFaces(inputImageEl, new faceapi.TinyFaceDetectorOptions())
        .withFaceLandmarks()
        .withFaceDescriptors();

      // Optionally resize results if your displayed image is smaller/bigger
      // const resizedDetections = faceapi.resizeResults(detections, displaySize);
      // But if the image is actual size, resizing not needed

      // Clear canvas + Draw
      const ctx = canvas.getContext('2d');
      ctx.clearRect(0, 0, canvas.width, canvas.height);

      detections.forEach(detection => {
        // Draw bounding box
        const box = detection.detection.box;
        ctx.strokeStyle = '#00FF00';
        ctx.lineWidth = 2;
        ctx.strokeRect(box.x, box.y, box.width, box.height);

        // Draw landmarks
        faceapi.draw.drawFaceLandmarks(canvas, [detection]);
      });

      statusEl.textContent = `Detected ${detections.length} face(s) in the image.`;
      console.log('Detections:', detections);
    });

    // -------------------------------------------------------
    // 3. Start Video
    // -------------------------------------------------------
    startVideoBtn.addEventListener('click', async () => {
      if (!modelsLoaded) {
        statusEl.textContent = 'Please load models first!';
        return;
      }
      if (videoStream) {
        statusEl.textContent = 'Video already started!';
        return;
      }

      try {
        videoStream = await navigator.mediaDevices.getUserMedia({ video: true });
        videoPreviewEl.srcObject = videoStream;
        videoPreviewEl.onloadedmetadata = () => {
          videoPreviewEl.play();
        };
        statusEl.textContent = 'Video stream started.';
      } catch (err) {
        console.error(err);
        statusEl.textContent = 'Error accessing webcam: ' + err;
      }
    });

    // -------------------------------------------------------
    // 4. Register a Face from Video
    // -------------------------------------------------------
    // Similar to blog logic: we capture a face from the video + store descriptor with label
    registerFaceBtn.addEventListener('click', async () => {
      if (!videoStream) {
        alert('Start video first!');
        return;
      }
      const label = labelInputEl.value.trim();
      if (!label) {
        alert('Please enter a label (e.g. "Alice") before registering.');
        return;
      }

      statusEl.textContent = 'Registering face for label: ' + label + '...';

      // Detect a single face in current video frame
      const detection = await faceapi.detectSingleFace(
        videoPreviewEl,
        new faceapi.TinyFaceDetectorOptions()
      ).withFaceLandmarks().withFaceDescriptor();

      if (!detection) {
        statusEl.textContent = 'No face detected. Try again.';
        return;
      }

      // Store descriptor in our dictionary
      if (!descriptorsByLabel[label]) {
        descriptorsByLabel[label] = [];
      }
      descriptorsByLabel[label].push(detection.descriptor);

      statusEl.textContent = `Registered face for "${label}". Total samples: ${descriptorsByLabel[label].length}`;
      console.log(`Registered descriptor for ${label}`, detection.descriptor);
    });

    // -------------------------------------------------------
    // 5. Create FaceMatcher
    // -------------------------------------------------------
    createMatcherBtn.addEventListener('click', () => {
      if (Object.keys(descriptorsByLabel).length === 0) {
        statusEl.textContent = 'No registered faces to create matcher.';
        return;
      }
      labeledDescriptors = [];
      for (const label in descriptorsByLabel) {
        labeledDescriptors.push(
          new faceapi.LabeledFaceDescriptors(label, descriptorsByLabel[label])
        );
      }
      // Create FaceMatcher with threshold (0.6 typical)
      faceMatcher = new faceapi.FaceMatcher(labeledDescriptors, 0.2);
      statusEl.textContent = 'FaceMatcher created with threshold=0.6!';
      console.log('Created FaceMatcher:', faceMatcher);
    });

    // -------------------------------------------------------
    // 6. Recognize in Real-Time (Video)
    // -------------------------------------------------------
    recognizeBtn.addEventListener('click', () => {
      if (!faceMatcher) {
        statusEl.textContent = 'Create matcher first!';
        return;
      }
      statusEl.textContent = 'Recognition started...';
      recognizeVideoLoop();
    });

    async function recognizeVideoLoop() {
      const canvas = videoOverlayEl;
      const ctx = canvas.getContext('2d');
      const w = videoPreviewEl.videoWidth;
      const h = videoPreviewEl.videoHeight;
      canvas.width = w;
      canvas.height = h;

      // Clear previous drawings
      ctx.clearRect(0, 0, w, h);

      // Detect faces
      const detections = await faceapi.detectAllFaces(
        videoPreviewEl,
        new faceapi.TinyFaceDetectorOptions()
      ).withFaceLandmarks().withFaceDescriptors();

      // For each face, find best match
      detections.forEach(d => {
        const box = d.detection.box;
        const bestMatch = faceMatcher.findBestMatch(d.descriptor);

        // Draw bounding box
        ctx.strokeStyle = '#00FF00';
        ctx.lineWidth = 2;
        ctx.strokeRect(box.x, box.y, box.width, box.height);

        // Draw label
        ctx.fillStyle = '#00FF00';
        ctx.font = '16px Arial';
        ctx.fillText(bestMatch.toString(), box.x, box.y - 5);
      });

      requestAnimationFrame(recognizeVideoLoop);
    }
  </script>
</body>
</html>
