<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Face Recognition (No Overlay, Just Label)</title>
  <!-- face-api.js from CDN -->
  <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

  <style>
    body {
      font-family: Arial, sans-serif;
      text-align: center;
      margin: 20px;
    }
    #status {
      margin-top: 10px;
      font-weight: bold;
      color: green;
    }
    .container {
      position: relative;
      display: inline-block;
      width: 640px;
      height: 480px;
      border: 1px solid #ccc;
    }
    /* The video is on the bottom layer */
    #videoElement {
      position: absolute;
      top: 0;
      left: 0;
      width: 640px;
      height: 480px;
      background: #000;
      z-index: 0;
    }
    /* The label (text) is on top of the video */
    #labelOnVideo {
      position: absolute;
      top: 0; 
      left: 0;
      width: 640px;
      height: auto;
      color: yellow;
      font-weight: bold;
      text-align: center;
      background: rgba(0,0,0,0.3);
      z-index: 10;
      pointer-events: none; /* clicks pass through */
    }
    #controls {
      margin-bottom: 1rem;
    }
    #labelInput {
      width: 120px;
    }
  </style>
</head>
<body>
  <h1>Face Recognition (Display Label Only)</h1>

  <div id="controls">
    <button id="loadModelsBtn">Load Models</button>
    <button id="startVideoBtn">Start Video</button>
    <input type="text" id="labelInput" placeholder="Label (e.g. Alice)" />
    <button id="registerBtn">Register Face</button>
    <button id="createMatcherBtn">Create Matcher</button>
    <button id="startRecBtn">Start Recognition</button>
  </div>

  <div id="status">Status messages will appear here...</div>

  <!-- Video container with a text label on top -->
  <div class="container">
    <video id="videoElement" autoplay muted playsinline></video>
    <div id="labelOnVideo"></div>
  </div>

  <script>
    // -----------------------------------------------------
    // Global State
    // -----------------------------------------------------
    let modelsLoaded = false;
    let videoStream = null;
    let faceMatcher = null;
    let descriptorsByLabel = {}; // { 'Alice': [Float32Array, ...], ... }

    // -----------------------------------------------------
    // DOM Elements
    // -----------------------------------------------------
    const loadModelsBtn  = document.getElementById('loadModelsBtn');
    const startVideoBtn  = document.getElementById('startVideoBtn');
    const registerBtn    = document.getElementById('registerBtn');
    const createMatcherBtn = document.getElementById('createMatcherBtn');
    const startRecBtn    = document.getElementById('startRecBtn');
    const labelInputEl   = document.getElementById('labelInput');
    const statusEl       = document.getElementById('status');
    const videoEl        = document.getElementById('videoElement');
    const labelOnVideoEl = document.getElementById('labelOnVideo');

    // -----------------------------------------------------
    // 1. Load Face-API Models
    // -----------------------------------------------------
    loadModelsBtn.addEventListener('click', async () => {
      statusEl.textContent = 'Loading models...';
      try {
        await faceapi.nets.tinyFaceDetector.loadFromUri('./models');
        await faceapi.nets.faceLandmark68Net.loadFromUri('./models');
        await faceapi.nets.faceRecognitionNet.loadFromUri('./models');
        // Or if you want SSD MobileNet:
        // await faceapi.nets.ssdMobilenetv1.loadFromUri('./models');

        modelsLoaded = true;
        statusEl.textContent = 'Models loaded successfully!';
        console.log('[DEBUG] Models loaded');
      } catch (err) {
        statusEl.textContent = 'Error loading models: ' + err;
        console.error(err);
      }
    });

    // -----------------------------------------------------
    // 2. Start Video
    // -----------------------------------------------------
    startVideoBtn.addEventListener('click', async () => {
      if (!modelsLoaded) {
        statusEl.textContent = 'Please load models first!';
        return;
      }
      if (videoStream) {
        statusEl.textContent = 'Video is already running.';
        return;
      }
      try {
        videoStream = await navigator.mediaDevices.getUserMedia({ video: true });
        videoEl.srcObject = videoStream;
        videoEl.onloadedmetadata = () => {
          videoEl.play();
          statusEl.textContent = 'Video stream started.';
          console.log('[DEBUG] Video started');
        };
      } catch (err) {
        console.error(err);
        statusEl.textContent = 'Error accessing webcam: ' + err;
      }
    });

    // -----------------------------------------------------
    // 3. Register a Face (From Video)
    // -----------------------------------------------------
    registerBtn.addEventListener('click', async () => {
      if (!videoStream) {
        alert('Please start the video first!');
        return;
      }
      const label = labelInputEl.value.trim();
      if (!label) {
        alert('Please enter a label before registering!');
        return;
      }

      statusEl.textContent = `Registering face for "${label}"...`;

      // Detect a single face from the video
      const detection = await faceapi.detectSingleFace(
        videoEl,
        new faceapi.TinyFaceDetectorOptions()
      ).withFaceLandmarks().withFaceDescriptor();

      if (!detection) {
        statusEl.textContent = 'No face detected, try again.';
        console.log('[DEBUG] No face detected for registration');
        return;
      }

      // Store descriptor
      if (!descriptorsByLabel[label]) {
        descriptorsByLabel[label] = [];
      }
      descriptorsByLabel[label].push(detection.descriptor);

      statusEl.textContent = `Registered face for "${label}". ` +
        `Samples: ${descriptorsByLabel[label].length}`;
      console.log(`[DEBUG] Registered descriptor for ${label}`);
    });

    // -----------------------------------------------------
    // 4. Create FaceMatcher
    // -----------------------------------------------------
    createMatcherBtn.addEventListener('click', () => {
      const labels = Object.keys(descriptorsByLabel);
      if (labels.length === 0) {
        statusEl.textContent = 'No faces registered. Register at least one face!';
        return;
      }
      const labeledFaceDescriptors = [];
      for (const label of labels) {
        labeledFaceDescriptors.push(
          new faceapi.LabeledFaceDescriptors(
            label,
            descriptorsByLabel[label]
          )
        );
      }
      // Default threshold=0.6
      faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.6);
      statusEl.textContent = 'FaceMatcher created (threshold=0.6).';
      console.log('[DEBUG] FaceMatcher created');
    });

    // -----------------------------------------------------
    // 5. Start Recognition
    // -----------------------------------------------------
    startRecBtn.addEventListener('click', () => {
      if (!faceMatcher) {
        statusEl.textContent = 'Please create matcher first!';
        return;
      }
      statusEl.textContent = 'Recognition running. Label displayed on top of the video.';
      console.log('[DEBUG] Recognition started');
      recognitionLoop();
    });

    // -----------------------------------------------------
    // 6. Real-Time Recognition Loop
    // -----------------------------------------------------
    async function recognitionLoop() {
      requestAnimationFrame(recognizeLoop);

      const video = videoEl;
      const videoWidth = video.videoWidth;
      const videoHeight = video.videoHeight;

      if (videoWidth === 0 || videoHeight === 0) return;

      // Get the displayed size of the video element
      const displaySize = {
        width: video.clientWidth,
        height: video.clientHeight
      };

      // Set canvas buffer size to match displayed size
      overlayEl.width = displaySize.width;
      overlayEl.height = displaySize.height;

      // Match dimensions for face-api.js
      faceapi.matchDimensions(overlayEl, displaySize);

      // Detect faces
      const detections = await faceapi.detectAllFaces(
        video,
        new faceapi.TinyFaceDetectorOptions()
      ).withFaceLandmarks().withFaceDescriptors();

      // Resize detections to the displayed size
      const resizedDetections = faceapi.resizeResults(detections, displaySize);

      // Clear overlay
      overlayCtx.clearRect(0, 0, overlayEl.width, overlayEl.height);

      // For each detection, find best match and draw
      resizedDetections.forEach(d => {
        const box = d.detection.box;
        const bestMatch = faceMatcher.findBestMatch(d.descriptor);

        // Draw bounding box
        overlayCtx.strokeStyle = '#00FF00';
        overlayCtx.lineWidth = 2;
        overlayCtx.strokeRect(box.x, box.y, box.width, box.height);

        // Draw label text
        overlayCtx.fillStyle = '#00FF00';
        overlayCtx.font = '16px Arial';
        overlayCtx.fillText(bestMatch.toString(), box.x, box.y - 5);
      });
    }
  </script>
</body>
</html>
