<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Face Recognition - Draw on Web</title>
  <!-- Load face-api.js (browser build) from CDN -->
  <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
  <style>
    body {
      font-family: Arial, sans-serif;
      text-align: center;
      margin: 20px;
    }
    #status {
      margin-top: 0.5rem;
      font-weight: bold;
      color: green;
    }
    .container {
      position: relative;
      display: inline-block;
      width: 640px;
      height: 480px;
      border: 1px solid #ccc;
    }
    /* Video placed at bottom (z-index: 0) */
    #videoElement {
      position: absolute;
      top: 0;
      left: 0;
      width: 640px;
      height: 480px;
      z-index: 0;
      background: #000;
    }
    /* Canvas overlay placed on top (z-index: 1) */
    #overlay {
      position: absolute;
      top: 0;
      left: 0;
      width: 640px;
      height: 480px;
      pointer-events: none; /* allow clicks to pass through */
      z-index: 1;
    }
    #controls {
      margin-bottom: 1rem;
    }
    #labelInput {
      width: 120px;
    }
  </style>
</head>
<body>
  <h1>Face Recognition (Drawing on the Web)</h1>
  <div id="controls">
    <button id="loadModelsBtn">Load Models</button>
    <button id="startVideoBtn">Start Video</button>
    <input type="text" id="labelInput" placeholder="Label (e.g. Alice)" />
    <button id="registerBtn">Register Face</button>
    <button id="createMatcherBtn">Create Matcher</button>
    <button id="startRecBtn">Start Recognition</button>
  </div>

  <div id="status">Status messages...</div>

  <!-- Video and overlay container -->
  <div class="container">
    <video id="videoElement" autoplay muted playsinline></video>
    <canvas id="overlay"></canvas>
  </div>

  <script>
    // -----------------------------------------------------
    // Global State
    // -----------------------------------------------------
    let modelsLoaded = false;
    let videoStream = null;
    let faceMatcher = null;

    // Example:
    // descriptorsByLabel = {
    //   'Alice': [Float32Array(128), Float32Array(128), ...],
    //   'Bob':   [Float32Array(128), ...]
    // }
    let descriptorsByLabel = {};

    // -----------------------------------------------------
    // DOM Elements
    // -----------------------------------------------------
    const statusEl       = document.getElementById('status');
    const loadModelsBtn  = document.getElementById('loadModelsBtn');
    const startVideoBtn  = document.getElementById('startVideoBtn');
    const registerBtn    = document.getElementById('registerBtn');
    const createMatcherBtn = document.getElementById('createMatcherBtn');
    const startRecBtn    = document.getElementById('startRecBtn');
    const labelInputEl   = document.getElementById('labelInput');

    const videoEl   = document.getElementById('videoElement');
    const overlayEl = document.getElementById('overlay');
    const overlayCtx = overlayEl.getContext('2d');

    // -----------------------------------------------------
    // 1. Load Models
    // -----------------------------------------------------
    loadModelsBtn.addEventListener('click', async () => {
      statusEl.textContent = 'Loading models...';
      try {
        await faceapi.nets.tinyFaceDetector.loadFromUri('./models');
        await faceapi.nets.faceLandmark68Net.loadFromUri('./models');
        await faceapi.nets.faceRecognitionNet.loadFromUri('./models');
        // Optionally: faceapi.nets.ssdMobilenetv1.loadFromUri('./models');
        modelsLoaded = true;
        statusEl.textContent = 'Models loaded successfully!';
        console.log('[DEBUG] Models loaded.');
      } catch (err) {
        statusEl.textContent = 'Error loading models: ' + err;
        console.error(err);
      }
    });

    // -----------------------------------------------------
    // 2. Start Video Stream
    // -----------------------------------------------------
    startVideoBtn.addEventListener('click', async () => {
      if (!modelsLoaded) {
        statusEl.textContent = 'Please load models first!';
        return;
      }
      if (videoStream) {
        statusEl.textContent = 'Video already running.';
        return;
      }
      try {
        videoStream = await navigator.mediaDevices.getUserMedia({ video: true });
        videoEl.srcObject = videoStream;
        videoEl.onloadedmetadata = () => {
          videoEl.play();
          statusEl.textContent = 'Video stream started.';
          console.log('[DEBUG] Video started.');
        };
      } catch (err) {
        console.error(err);
        statusEl.textContent = 'Error accessing webcam: ' + err;
      }
    });

    // -----------------------------------------------------
    // 3. Register Face
    // -----------------------------------------------------
    registerBtn.addEventListener('click', async () => {
      if (!videoStream) {
        alert('Please start video first!');
        return;
      }
      const label = labelInputEl.value.trim();
      if (!label) {
        alert('Please enter a label before registering.');
        return;
      }

      statusEl.textContent = `Registering face for label: ${label}...`;

      // Detect a single face from the current video frame
      const detection = await faceapi.detectSingleFace(
        videoEl,
        new faceapi.TinyFaceDetectorOptions()
      ).withFaceLandmarks().withFaceDescriptor();

      if (!detection) {
        statusEl.textContent = 'No face detected. Try again.';
        console.log('[DEBUG] No face detected for registration.');
        return;
      }

      // Save descriptor in descriptorsByLabel
      if (!descriptorsByLabel[label]) {
        descriptorsByLabel[label] = [];
      }
      descriptorsByLabel[label].push(detection.descriptor);

      statusEl.textContent = `Registered face for "${label}". ` +
        `Samples: ${descriptorsByLabel[label].length}`;
      console.log(`[DEBUG] Registered descriptor for ${label}`);
    });

    // -----------------------------------------------------
    // 4. Create FaceMatcher
    // -----------------------------------------------------
    createMatcherBtn.addEventListener('click', () => {
      const labels = Object.keys(descriptorsByLabel);
      if (labels.length === 0) {
        statusEl.textContent = 'No faces registered. Please register some faces first.';
        return;
      }
      // Build an array of LabeledFaceDescriptors
      const labeledFaceDescriptors = [];
      for (const label of labels) {
        labeledFaceDescriptors.push(
          new faceapi.LabeledFaceDescriptors(
            label,
            descriptorsByLabel[label] // array of Float32Array
          )
        );
      }
      // FaceMatcher with default threshold=0.6
      faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.6);
      statusEl.textContent = 'FaceMatcher created with threshold=0.6.';
      console.log('[DEBUG] FaceMatcher created:', faceMatcher);
    });

    // -----------------------------------------------------
    // 5. Start Real-Time Recognition
    // -----------------------------------------------------
    startRecBtn.addEventListener('click', () => {
      if (!faceMatcher) {
        statusEl.textContent = 'Please create matcher first!';
        return;
      }
      statusEl.textContent = 'Recognition started. Check bounding boxes on video...';
      console.log('[DEBUG] Starting recognition loop.');
      recognizeLoop();
    });

    async function recognizeLoop() {
      // Use requestAnimationFrame for continuous detection
      requestAnimationFrame(recognizeLoop);

      // Match the canvas size to the video
      const w = videoEl.videoWidth;
      const h = videoEl.videoHeight;
      overlayEl.width = w;
      overlayEl.height = h;

      // Clear overlay
      overlayCtx.clearRect(0, 0, w, h);

      // Detect faces
      const detections = await faceapi.detectAllFaces(
        videoEl,
        new faceapi.TinyFaceDetectorOptions()
      ).withFaceLandmarks().withFaceDescriptors();

      // If no detections, skip drawing
      if (!detections.length) return;

      // For each detection, find best match
      detections.forEach(d => {
        const box = d.detection.box;
        const bestMatch = faceMatcher.findBestMatch(d.descriptor);
        console.log(`[DEBUG] bestMatch: ${bestMatch.toString()}`); // e.g. "Alice (distance: 0.53)"

        // Draw bounding box
        overlayCtx.strokeStyle = '#00FF00';
        overlayCtx.lineWidth = 2;
        overlayCtx.strokeRect(box.x, box.y, box.width, box.height);

        // Draw label text
        overlayCtx.fillStyle = '#00FF00';
        overlayCtx.font = '16px Arial';
        overlayCtx.fillText(bestMatch.toString(), box.x, box.y - 5);
      });
    }
  </script>
</body>
</html>
